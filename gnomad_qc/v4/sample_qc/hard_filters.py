import argparse
import logging

import hail as hl

from gnomad.resources.grch38.reference_data import telomeres_and_centromeres
from gnomad.sample_qc.filtering import compute_stratified_sample_qc
from gnomad.utils.annotations import bi_allelic_expr
from gnomad.utils.filtering import add_filters_expr

from gnomad_qc.v4.resources.basics import (
    calling_intervals,
    get_checkpoint_path,
    get_gnomad_v4_vds,
    get_logging_path,
    gnomad_v4_testset,
    gnomad_v4_testset_meta,
)
from gnomad_qc.v4.resources.meta import project_meta
from gnomad_qc.v4.resources.sample_qc import (
    fingerprinting_failed,
    get_sample_qc,
    hard_filtered_samples,
    interval_coverage,
    sex,
)

logging.basicConfig(format="%(levelname)s (%(name)s %(lineno)s): %(message)s")
logger = logging.getLogger("hard_filters")
logger.setLevel(logging.INFO)


def compute_sample_qc(n_partitions: int = 500, test: bool = False) -> hl.Table:
    """
    Perform sample QC on the raw split matrix table using `compute_stratified_sample_qc`.

    :param n_partitions: Number of partitions to write the output sample QC HT to.
    :param test: Whether to use the gnomAD v4 test dataset. Default is to use the full dataset.
    :return: Table containing sample QC metrics.
    """
    logger.info("Computing sample QC")
    vds = get_gnomad_v4_vds(split=True, remove_hard_filtered_samples=False, test=test)
    vds = hl.vds.filter_chromosomes(vds, keep_autosomes=True)

    # Remove centromeres and telomeres in case they were included
    vds = hl.vds.filter_intervals(
        vds, intervals=telomeres_and_centromeres.ht(), keep=False
    )

    sample_qc_ht = compute_stratified_sample_qc(
        vds,
        strata={
            "bi_allelic": bi_allelic_expr(vds.variant_data),
            "multi_allelic": ~bi_allelic_expr(vds.variant_data),
        },
        tmp_ht_prefix=get_sample_qc(test=test).path[:-3],
        gt_col="GT",
    )

    return sample_qc_ht.repartition(n_partitions)


def compute_hard_filters(
    coverage_mt: hl.MatrixTable,
    include_sex_filter: bool = False,
    min_cov: int = 15,
    max_n_snp: float = 3.75e6,
    min_n_snp: float = 2.4e6,
    max_n_singleton: float = 1e5,
    max_r_het_hom_var: float = 3.3,
    max_contamination: float = 0.05,
    max_chimera: float = 0.05,
    min_n_over_gq: float = 2.7e9,
    min_gq: int = 20,
    min_n_over_dp: float = 2.7e9,
    min_dp: int = 10,
    test: bool = False,
) -> hl.Table:
    """
    Apply hard filters to samples and return a Table with the filtered samples and the reason for filtering.

    If `include_sex_filter` is True, this function expects a sex inference Table generated by
    `sex_inference.py --impute-sex`.

    :param coverage_mt: MatrixTable containing the per interval per sample coverage statistics.
    :param include_sex_filter: If sex inference should be used in filtering.
    :param min_cov: Filtering threshold to use for chr20 min mean coverage.
    :param max_n_snp: Filtering threshold to use for the max number of SNPs.
    :param min_n_snp: Filtering threshold to use for the min number of SNPs.
    :param max_n_singleton: Filtering threshold to use for the max number of singletons.
    :param max_r_het_hom_var: Filtering threshold to use for the max ratio of heterozygotes to alternate homozygotes
    :param max_contamination: Filtering threshold to use for max contamination (this is a proportion not a
        percecnt, e.g. 5% == 0.05, %5 != 5).
    :param max_chimera: Filtering threshold to use for max chimera (this is a proportion not a percent,
        e.g. 5% == 0.05, %5 != 5).
    :param min_n_over_gq: Minimum number of bases with a GQ over the filtering threshold. Default is 2.7e9.
    :param min_gq: GQ threshold to use for filtering. Default is 20.
    :param min_n_over_dp: Minimum number of bases with a DP over the filtering threshold. Default is 2.7e9.
    :param min_dp: DP threshold to use for filtering. Default is 10.
    :param test: Whether to use the gnomAD v4 test dataset. Default is to use the full dataset.
    :return: Table of hard filtered samples.
    """
    ht = get_gnomad_v4_vds(
        remove_hard_filtered_samples=False, test=test
    ).variant_data.cols()
    ht = ht.annotate_globals(
        hard_filter_cutoffs=hl.struct(
            min_cov=min_cov,
            # TODO: See TODO below
            # max_n_snp=max_n_snp,
            # min_n_snp=min_n_snp,
            # max_n_singleton=max_n_singleton,
            # max_r_het_hom_var=max_r_het_hom_var,
            max_contamination=max_contamination,
            max_chimera=max_chimera,
            # min_n_over_gq=min_n_over_gq,
            # min_gq=min_gq,
            # min_n_over_dp=min_n_over,
            min_dp=min_dp,
        ),
    )
    hard_filters = dict()

    # Flag samples failing fingerprinting
    fp_ht = fingerprinting_failed.ht()
    hard_filters["failed_fingerprinting"] = hl.is_defined(fp_ht[ht.key])

    # Flag extreme raw bi-allelic sample QC outliers
    # TODO: Do we still want all of these? They are the metrics and cutoffs used for gnomAD v3 and v3.1, my guess is we
    #  will use none of these filters as hard-filters, but it will be good to look at the distributions to determine
    #  this. There could also be other metrics we want to use instead, but will not know until we have distributions on
    #  the full set.
    # bi_allelic_qc_ht = get_sample_qc("bi_allelic", test=test).ht()
    # Convert tuples to lists so we can find the index of the passed threshold
    # gq_bins = [
    #    hl.eval(bi_allelic_qc_ht.gq_bins[i])
    #    for i in range(len(bi_allelic_qc_ht.gq_bins))
    # ]
    # dp_bins = [
    #    hl.eval(bi_allelic_qc_ht.dp_bins[i])
    #    for i in range(len(bi_allelic_qc_ht.dp_bins))
    # ]
    # bi_allelic_qc_struct = bi_allelic_qc_ht[ht.key]
    # hard_filters["sample_qc_metrics"] = (
    #    (bi_allelic_qc_struct.n_snp > max_n_snp)
    #    | (bi_allelic_qc_struct.n_snp < min_n_snp)
    #    | (bi_allelic_qc_struct.n_singleton > max_n_singleton)
    #    | (bi_allelic_qc_struct.r_het_hom_var > max_r_het_hom_var)
    #    | (
    #        bi_allelic_qc_struct.bases_over_gq[
    #            gq_bins.index(min_gq)
    #        ]
    #        < min_n_over_gq
    #    )
    #    | (
    #        bi_allelic_qc_struct.bases_over_dp[
    #            dp_bins.index(min_dp)
    #        ]
    #        < min_n_over_dp
    #    )
    # )

    # Flag samples that fail bam metric thresholds
    if test:
        project_meta_ht = gnomad_v4_testset_meta.ht()
        # Use the gnomAD v4 test dataset's `rand_sampling_meta` annotation to get the bam metrics needed for hard filtering
        # This annotation includes all of the metadata for the random samples chosen for the test dataset
        bam_metrics_struct = project_meta_ht[ht.key].rand_sampling_meta
        bam_metrics_struct = bam_metrics_struct.annotate(
            contam_rate=bam_metrics_struct.freemix,
            chimeras_rate=bam_metrics_struct.pct_chimeras,
        )
    else:
        project_meta_ht = project_meta.ht()
        bam_metrics_struct = project_meta_ht[ht.key].bam_metrics

    hard_filters["contamination"] = bam_metrics_struct.contam_rate > max_contamination
    hard_filters["chimera"] = bam_metrics_struct.chimeras_rate > max_chimera

    # Flag low-coverage samples using mean coverage on chromosome 20
    coverage_mt = coverage_mt.filter_rows(coverage_mt.interval.start.contig == "chr20")
    coverage_ht = coverage_mt.select_cols(
        chr20_mean_dp=hl.agg.sum(coverage_mt.sum_dp)
        / hl.agg.sum(coverage_mt.interval_size)
    ).cols()

    hard_filters["low_coverage"] = coverage_ht[ht.key].chr20_mean_dp < min_cov

    if include_sex_filter:
        sex_struct = sex.ht()[ht.key]
        # Remove samples with ambiguous sex assignments
        hard_filters["ambiguous_sex"] = sex_struct.sex_karyotype == "ambiguous"
        hard_filters[
            "sex_aneuploidy"
        ] = ~hl.set(  # pylint: disable=invalid-unary-operand-type
            {"ambiguous", "XX", "XY"}
        ).contains(
            sex_struct.sex_karyotype
        )

    ht = ht.annotate(hard_filters=add_filters_expr(filters=hard_filters))

    # Keep samples failing hard filters
    ht = ht.filter(hl.len(ht.hard_filters) > 0)
    return ht


def main(args):
    hl.init(log="/gnomad_hard_filters.log", default_reference="GRCh38")
    calling_interval_name = args.calling_interval_name
    calling_interval_padding = args.calling_interval_padding
    test = args.test

    try:
        if args.sample_qc:
            compute_sample_qc(
                n_partitions=args.sample_qc_n_partitions, test=test
            ).write(get_sample_qc(test=test).path, overwrite=args.overwrite)

        if args.compute_coverage:
            if test:
                logger.info("Loading test VDS...")
                vds = gnomad_v4_testset.vds()
            else:
                logger.info("Loading full v4 VDS...")
                vds = get_gnomad_v4_vds(remove_hard_filtered_samples=False)

            logger.info(
                "Loading calling intervals: %s with padding of %d...",
                calling_interval_name,
                calling_interval_padding,
            )
            ht = calling_intervals(calling_interval_name, calling_interval_padding).ht()
            mt = hl.vds.interval_coverage(vds, intervals=ht)
            mt = mt.annotate_globals(
                calling_interval_name=calling_interval_name,
                calling_interval_padding=calling_interval_padding,
            )
            mt.write(
                get_checkpoint_path(
                    f"test_interval_coverage.{calling_interval_name}.pad{calling_interval_padding}",
                    mt=True,
                )
                if test
                else interval_coverage.path,
                overwrite=args.overwrite,
            )

        if args.compute_hard_filters:
            # TODO: Determine cutoffs by visual inspection of the metrics, and modify defaults to match
            if test:
                coverage_mt = hl.read_matrix_table(
                    get_checkpoint_path(
                        f"test_interval_coverage.{calling_interval_name}.pad{calling_interval_padding}",
                        mt=True,
                    )
                )
            else:
                coverage_mt = interval_coverage.mt()

            compute_hard_filters(
                coverage_mt,
                args.include_sex_filter,
                args.min_cov,
                args.max_n_snp,
                args.min_n_snp,
                args.max_n_singleton,
                args.max_r_het_hom_var,
                args.max_contamination,
                args.max_chimera,
                args.min_n_over_gq,
                args.min_gq,
                args.min_n_over_dp,
                args.min_dp,
                test,
            ).write(hard_filtered_samples.path, overwrite=args.overwrite)

    finally:
        logger.info("Copying log to logging bucket...")
        hl.copy_log(get_logging_path("hard_filters"))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--overwrite",
        help="Overwrite all Matrixtables/Tables. (default: False).",
        action="store_true",
    )
    parser.add_argument(
        "--test",
        help="Use the v4 test dataset instead of the full dataset.",
        action="store_true",
    )
    parser.add_argument(
        "--sample-qc", help="Compute Hail's VDS sample QC metrics.", action="store_true"
    )
    parser.add_argument(
        "--sample-qc-n-partitions",
        help="Number of desired partitions for the sample QC output Table.",
        default=500,
        type=int,
    )
    parser.add_argument(
        "--compute-coverage",
        help="Compute per interval coverage metrics using Hail's vds.interval_coverage method.",
        action="store_true",
    )
    parser.add_argument(
        "--calling-interval-name",
        help="Name of calling intervals to use for interval coverage. One of: 'ukb', 'broad', or 'intersection'.",
        type=str,
        choices=["ukb", "broad", "intersection"],
        default="intersection",
    )
    parser.add_argument(
        "--calling-interval-padding",
        help="Number of base pair padding to use on the calling intervals. One of 0 or 50 bp.",
        type=int,
        choices=[0, 50],
        default=50,
    )
    parser.add_argument(
        "--compute-hard-filters",
        help="Computes samples to be hard-filtered. NOTE: Cutoffs should be determined by visual inspection of the metrics.",
        action="store_true",
    )
    parser.add_argument(
        "--include-sex-filter",
        help="If sex filters should be included in hard filtering.",
        action="store_true",
    )
    parser.add_argument_group()
    hard_filter_args = parser.add_argument_group(
        "Hard-filter cutoffs", "Arguments used for hard-filter cutoffs."
    )
    hard_filter_args.add_argument(
        "--min-cov",
        help="Minimum coverage for inclusion when computing hard-filters.",
        default=15,
        type=int,
    )
    hard_filter_args.add_argument(
        "--max-n-snp",
        type=float,
        default=3.75e6,
        help="Filtering threshold to use for the maximum number of SNPs. Default is 3.75e6.",
    )
    hard_filter_args.add_argument(
        "--min-n-snp",
        type=float,
        default=2.4e6,
        help="Filtering threshold to use for the minimum number of SNPs. Default is 2.4e6.",
    )
    hard_filter_args.add_argument(
        "--max-n-singleton",
        type=float,
        default=1e5,
        help="Filtering threshold to use for the max number of singletons. Default is 1e5.",
    )
    hard_filter_args.add_argument(
        "-max-r-het-hom-var",
        type=float,
        default=3.3,
        help="Filtering threshold to use for the max ratio of heterozygotes to alternate homozygotes. Default is 3.3.",
    )
    hard_filter_args.add_argument(
        "--max-contamination",
        default=0.05,
        type=float,
        help="Filtering threshold to use for max contamination (this is a proportion not percent, e.g. 5% == 0.05, %5 != 5). Default is 0.05.",
    )
    hard_filter_args.add_argument(
        "--max-chimera",
        type=float,
        default=0.05,
        help="Filtering threshold to use for max chimera (this is a proportion not a percent, e.g. 5% == 0.05, %5 != 5). Default is 0.05.",
    )
    hard_filter_args.add_argument(
        "--min-n-over-gq",
        type=float,
        help="Minimum number of bases with a GQ over the filtering threshold. Default is 2.7e9.",
        default=2.7e9,
    )
    hard_filter_args.add_argument(
        "--min-gq",
        type=int,
        help="Minimum GQ threshold to use for filtering.",
        choices=[0, 20, 60],
        default=20,
    )
    hard_filter_args.add_argument(
        "--min-n-over-dp",
        type=float,
        help="Minimum number of bases with a DP over the filtering threshold. Default is 2.7e9.",
        default=2.7e9,
    )
    hard_filter_args.add_argument(
        "--min-dp",
        type=int,
        help="Minimum DP threshold to use for filternig.",
        choices=[0, 1, 10, 20, 30],
        default=10,
    )

    main(parser.parse_args())
