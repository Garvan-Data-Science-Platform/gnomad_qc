Index: gnomad_qc/v4/annotations/generate_variant_qc_annotations.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Script to generate annotations for variant QC on gnomAD v4.\"\"\"\n\nimport argparse\nimport logging\n\nimport hail as hl\nfrom gnomad.assessment.validity_checks import count_vep_annotated_variants_per_interval\nfrom gnomad.resources.grch38.reference_data import ensembl_interval\nfrom gnomad.utils.annotations import add_variant_type, annotate_allele_info\nfrom gnomad.utils.slack import slack_notifications\nfrom gnomad.utils.sparse_mt import (\n    default_compute_info,\n    split_info_annotation,\n    split_lowqual_annotation,\n)\nfrom gnomad.utils.vcf import adjust_vcf_incompatible_types\nfrom gnomad.utils.vep import vep_or_lookup_vep\n\nfrom gnomad_qc.resource_utils import (\n    PipelineResourceCollection,\n    PipelineStepResourceCollection,\n)\nfrom gnomad_qc.slack_creds import slack_token\nfrom gnomad_qc.v4.resources.annotations import (\n    get_info,\n    get_vep,\n    info_vcf_path,\n    validate_vep_path,\n)\nfrom gnomad_qc.v4.resources.basics import get_gnomad_v4_vds\nfrom gnomad_qc.v4.resources.constants import CURRENT_VERSION\n\nlogging.basicConfig(\n    format=\"%(asctime)s (%(name)s %(lineno)s): %(message)s\",\n    datefmt=\"%m/%d/%Y %I:%M:%S %p\",\n)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\ndef extract_as_pls(\n    lpl_expr: hl.expr.ArrayExpression,\n    allele_idx: hl.expr.Int32Expression,\n) -> hl.expr.ArrayExpression:\n    \"\"\"\n    Extract PLs for a specific allele from an LPL array expression.\n\n    PL/LPL represents the normalized Phred-scaled likelihoods of the possible\n    genotypes from all considered alleles (or local alleles).\n\n    If three alleles are considered, LPL genotype indexes are:\n    [0/0, 0/1, 1/1, 0/2, 1/2, 2/2].\n\n    If we want to extract the PLs for each alternate allele, we need to extract:\n        - allele 1: [0/0, 0/1, 1/1]\n        - allele 2: [0/0, 0/2, 2/2]\n\n    Example:\n        - LPL: [138, 98, 154, 26, 0, 14]\n        - Extract allele 1 PLs: [0/0, 0/1, 1/1] -> [138, 98, 154]\n        - Extract allele 2 PLs: [0/0, 0/2, 2/2] -> [138, 26, 14]\n\n    :param lpl_expr: LPL ArrayExpression.\n    :param allele_idx: The index of the alternate allele to extract PLs for.\n    :return: ArrayExpression of PLs for the specified allele.\n    \"\"\"\n    calls_to_keep = hl.array(\n        [hl.call(0, 0), hl.call(0, allele_idx), hl.call(allele_idx, allele_idx)]\n    )\n    return calls_to_keep.map(lambda c: lpl_expr[c.unphased_diploid_gt_index()])\n\n\ndef recompute_as_qualapprox_from_lpl(mt: hl.MatrixTable) -> hl.expr.ArrayExpression:\n    \"\"\"\n    Recompute AS_QUALapprox from LPL.\n\n    QUALapprox is the (Phred-scaled) probability that all reads at the site are hom-ref,\n    so QUALapprox is PL[0]. To get the QUALapprox for just one allele, pull out the\n    PLs for just that allele, then normalize by subtracting the smallest element from\n    all the entries (so the best genotype is 0) and then use the normalized PL[0]\n    value for that allele's QUALapprox.\n\n    .. note::\n\n        - The first element of AS_QUALapprox is always None.\n        - If the allele is a star allele, we set QUALapprox to 0.\n\n    Example:\n        Starting Values:\n            - alleles: [‘G’, ‘*’, ‘A’, ‘C’, ‘GCTT’, ‘GT’, ‘T’]\n            - LGT: 1/2\n            - LA: [0, 1, 6]\n            - LPL: [138, 98, 154, 26, 0, 14]\n            - QUALapprox: 138\n\n        Use `extract_as_pls` to get PLs for each allele:\n            - allele 1: [138, 98, 154]\n            - allele 2: [138, 26, 14]\n\n        Normalize PLs by subtracting the smallest element from all the PLs:\n            - allele 1: [138-98, 98-98, 154-98] -> [40, 0, 56]\n            - allele 2: [138-14, 26-14, 14-14] -> [124, 12, 0]\n\n        Use the first element of the allele specific PLs to generate AS_QUALapprox:\n        [None, 40, 124]\n\n        Set QUALapprox to 0 for the star allele: [None, 0, 124]\n\n    :param mt: Input MatrixTable.\n    :return: AS_QUALapprox ArrayExpression recomputed from LPL.\n    \"\"\"\n    return hl.enumerate(mt.LA).map(\n        lambda i: (\n            hl.case()\n            .when(mt.alleles[i[1]] == \"*\", 0)\n            .when(\n                i[0] > 0,\n                hl.bind(\n                    lambda pl_0: hl.if_else((mt.GQ < 2) & (pl_0 == 1), 0, pl_0),\n                    hl.bind(lambda x: x[0] - hl.min(x), extract_as_pls(mt.LPL, i[0])),\n                ),\n            )\n            .or_missing()\n        )\n    )\n\n\ndef run_compute_info(mt: hl.MatrixTable, test: bool = False) -> hl.Table:\n    \"\"\"\n    Run compute info on a MatrixTable.\n\n    ..note::\n\n        Adds a fix for AS_QUALapprox by recomputing from LPL because some were found to\n        have different lengths than LA.\n\n    :param mt: Input MatrixTable.\n    :param test: Whether to use the test dataset. Default is False.\n    :return: Table with info annotations.\n    \"\"\"\n    mt = mt.annotate_entries(\n        gvcf_info=mt.gvcf_info.annotate(\n            AS_QUALapprox=recompute_as_qualapprox_from_lpl(mt)\n        )\n    )\n\n    if test:\n        unrelated_expr = ~mt.meta.rand_sampling_meta.related\n    else:\n        unrelated_expr = ~mt.meta.sample_filters.relatedness_filters.related\n\n    return default_compute_info(\n        mt,\n        site_annotations=True,\n        as_annotations=True,\n        ac_filter_groups={\"release\": mt.meta.release, \"unrelated\": unrelated_expr},\n    )\n\n\ndef split_info(info_ht: hl.Table) -> hl.Table:\n    \"\"\"\n    Generate an info Table with split multi-allelic sites from the multi-allelic info Table.\n\n    .. note::\n\n        gnomad_methods' `annotate_allele_info` splits multi-allelic sites before the\n        `info` annotation is split to ensure that all sites in the returned Table are\n        annotated with allele info.\n\n    :param info_ht: Info Table with unsplit multi-allelics.\n    :return: Info Table with split multi-allelics.\n    \"\"\"\n    info_ht = annotate_allele_info(info_ht)\n    info_ht = info_ht.annotate(\n        info=info_ht.info.annotate(\n            **split_info_annotation(info_ht.info, info_ht.a_index),\n        ),\n        AS_lowqual=split_lowqual_annotation(info_ht.AS_lowqual, info_ht.a_index),\n    )\n\n    return info_ht\n\n\ndef get_variant_qc_annotation_resources(\n    test: bool, overwrite: bool\n) -> PipelineResourceCollection:\n    \"\"\"\n    Get PipelineResourceCollection for all resources needed in the variant QC annotation pipeline.\n\n    :param test: Whether to gather all resources for testing.\n    :param overwrite: Whether to overwrite resources if they exist.\n    :return: PipelineResourceCollection containing resources for all steps of the\n        variant QC annotation pipeline.\n    \"\"\"\n    # Initialize variant QC annotation pipeline resource collection.\n    ann_pipeline = PipelineResourceCollection(\n        pipeline_name=\"variant_qc_annotation\",\n        overwrite=overwrite,\n    )\n\n    # Create resource collection for each step of the variant QC annotation pipeline.\n    compute_info = PipelineStepResourceCollection(\n        \"--compute-info\",\n        output_resources={\"info_ht\": get_info(split=False, test=test)},\n    )\n    split_info_ann = PipelineStepResourceCollection(\n        \"--split-info\",\n        output_resources={\"split_info_ht\": get_info(test=test)},\n        pipeline_input_steps=[compute_info],\n    )\n    export_info_vcf = PipelineStepResourceCollection(\n        \"--export-info-vcf\",\n        output_resources={\"info_vcf\": info_vcf_path(test=test)},\n        pipeline_input_steps=[compute_info],\n    )\n    run_vep = PipelineStepResourceCollection(\n        \"--run-vep\",\n        output_resources={\n            \"vep_ht\": get_vep(version=CURRENT_VERSION, data_type=\"exomes\", test=test)\n        },\n    )\n    validate_vep = PipelineStepResourceCollection(\n        \"--validate-vep\",\n        output_resources={\"vep_count_ht\": validate_vep_path(test=test)},\n        pipeline_input_steps=[run_vep],\n    )\n\n    # Add all steps to the variant QC annotation pipeline resource collection.\n    ann_pipeline.add_steps(\n        {\n            \"compute_info\": compute_info,\n            \"split_info\": split_info_ann,\n            \"export_info_vcf\": export_info_vcf,\n            \"run_vep\": run_vep,\n            \"validate_vep\": validate_vep,\n        }\n    )\n\n    return ann_pipeline\n\n\ndef main(args):\n    \"\"\"Generate all variant annotations needed for variant QC.\"\"\"\n    hl.init(\n        default_reference=\"GRCh38\",\n        log=\"/variant_qc_annotations.log\",\n        tmp_dir=\"gs://gnomad-tmp-4day\",\n    )\n    test_dataset = args.test_dataset\n    test_n_partitions = args.test_n_partitions\n    test = test_dataset or test_n_partitions\n    run_vep = args.run_vep\n    overwrite = args.overwrite\n    resources = get_variant_qc_annotation_resources(test=test, overwrite=overwrite)\n    mt = get_gnomad_v4_vds(\n        test=test_dataset,\n        high_quality_only=False if run_vep else True,\n        # Keep control/truth samples because they are used in variant QC.\n        keep_controls=False if run_vep else True,\n        annotate_meta=False if run_vep else True,\n    ).variant_data\n\n    if test_n_partitions:\n        mt = mt._filter_partitions(range(test_n_partitions))\n\n    if args.compute_info:\n        # TODO: is there any reason to also compute info per platform?\n        res = resources.compute_info\n        res.check_resource_existence()\n        ht = run_compute_info(mt, test=test_dataset)\n        ht.write(res.info_ht.path, overwrite=overwrite)\n\n    if args.split_info:\n        res = resources.split_info\n        res.check_resource_existence()\n        split_info(res.info_ht.ht()).write(res.split_info_ht.path, overwrite=overwrite)\n\n    if args.export_info_vcf:\n        res = resources.export_info_vcf\n        res.check_resource_existence()\n        hl.export_vcf(adjust_vcf_incompatible_types(res.info_ht.ht()), res.info_vcf)\n\n    if run_vep:\n        res = resources.run_vep\n        res.check_resource_existence()\n        ht = hl.split_multi(mt.rows())\n        ht = vep_or_lookup_vep(ht, vep_version=args.vep_version)\n        ht.write(res.vep_ht.path, overwrite=args.overwrite)\n\n    if args.validate_vep:\n        res = resources.validate_vep\n        res.check_resource_existence()\n        count_ht = count_vep_annotated_variants_per_interval(\n            res.vep_ht.ht(), ensembl_interval.ht()\n        )\n        count_ht.write(res.vep_count_ht.path, overwrite=args.overwrite)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--slack-channel\", help=\"Slack channel to post results and notifications to.\"\n    )\n    parser.add_argument(\"--overwrite\", help=\"Overwrite data\", action=\"store_true\")\n    parser.add_argument(\n        \"--test-dataset\", help=\"Use the test dataset as input.\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"--test-n-partitions\",\n        help=\"Use only 2 partitions of the VDS as input for testing purposes.\",\n        nargs=\"?\",\n        const=2,\n        type=int,\n    )\n    parser.add_argument(\"--compute-info\", help=\"Compute info HT.\", action=\"store_true\")\n    parser.add_argument(\"--split-info\", help=\"Split info HT.\", action=\"store_true\")\n    parser.add_argument(\n        \"--export-info-vcf\", help=\"Export info as VCF.\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"--run-vep\", help=\"Generates vep annotations.\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"--validate-vep\",\n        help=(\n            \"Validate that variants in protein-coding genes are correctly annotated by\"\n            \" VEP.\"\n        ),\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--vep-version\",\n        help=\"Version of VEPed context Table to use in vep_or_lookup_vep.\",\n        default=\"105\",\n    )\n\n    args = parser.parse_args()\n\n    if args.slack_channel:\n        with slack_notifications(slack_token, args.slack_channel):\n            main(args)\n    else:\n        main(args)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/gnomad_qc/v4/annotations/generate_variant_qc_annotations.py b/gnomad_qc/v4/annotations/generate_variant_qc_annotations.py
--- a/gnomad_qc/v4/annotations/generate_variant_qc_annotations.py	(revision a5ce907e18f8fccee7a8897075dce99e4e4bb314)
+++ b/gnomad_qc/v4/annotations/generate_variant_qc_annotations.py	(date 1694811697024)
@@ -15,6 +15,7 @@
 )
 from gnomad.utils.vcf import adjust_vcf_incompatible_types
 from gnomad.utils.vep import vep_or_lookup_vep
+from gnomad.variant_qc.pipeline import generate_sib_stats, generate_trio_stats
 
 from gnomad_qc.resource_utils import (
     PipelineResourceCollection,
@@ -23,12 +24,13 @@
 from gnomad_qc.slack_creds import slack_token
 from gnomad_qc.v4.resources.annotations import (
     get_info,
+    get_sib_stats,
+    get_trio_stats,
     get_vep,
     info_vcf_path,
-    validate_vep_path,
 )
 from gnomad_qc.v4.resources.basics import get_gnomad_v4_vds
-from gnomad_qc.v4.resources.constants import CURRENT_VERSION
+from gnomad_qc.v4.resources.sample_qc import pedigree, relatedness
 
 logging.basicConfig(
     format="%(asctime)s (%(name)s %(lineno)s): %(message)s",
@@ -38,125 +40,6 @@
 logger.setLevel(logging.INFO)
 
 
-def extract_as_pls(
-    lpl_expr: hl.expr.ArrayExpression,
-    allele_idx: hl.expr.Int32Expression,
-) -> hl.expr.ArrayExpression:
-    """
-    Extract PLs for a specific allele from an LPL array expression.
-
-    PL/LPL represents the normalized Phred-scaled likelihoods of the possible
-    genotypes from all considered alleles (or local alleles).
-
-    If three alleles are considered, LPL genotype indexes are:
-    [0/0, 0/1, 1/1, 0/2, 1/2, 2/2].
-
-    If we want to extract the PLs for each alternate allele, we need to extract:
-        - allele 1: [0/0, 0/1, 1/1]
-        - allele 2: [0/0, 0/2, 2/2]
-
-    Example:
-        - LPL: [138, 98, 154, 26, 0, 14]
-        - Extract allele 1 PLs: [0/0, 0/1, 1/1] -> [138, 98, 154]
-        - Extract allele 2 PLs: [0/0, 0/2, 2/2] -> [138, 26, 14]
-
-    :param lpl_expr: LPL ArrayExpression.
-    :param allele_idx: The index of the alternate allele to extract PLs for.
-    :return: ArrayExpression of PLs for the specified allele.
-    """
-    calls_to_keep = hl.array(
-        [hl.call(0, 0), hl.call(0, allele_idx), hl.call(allele_idx, allele_idx)]
-    )
-    return calls_to_keep.map(lambda c: lpl_expr[c.unphased_diploid_gt_index()])
-
-
-def recompute_as_qualapprox_from_lpl(mt: hl.MatrixTable) -> hl.expr.ArrayExpression:
-    """
-    Recompute AS_QUALapprox from LPL.
-
-    QUALapprox is the (Phred-scaled) probability that all reads at the site are hom-ref,
-    so QUALapprox is PL[0]. To get the QUALapprox for just one allele, pull out the
-    PLs for just that allele, then normalize by subtracting the smallest element from
-    all the entries (so the best genotype is 0) and then use the normalized PL[0]
-    value for that allele's QUALapprox.
-
-    .. note::
-
-        - The first element of AS_QUALapprox is always None.
-        - If the allele is a star allele, we set QUALapprox to 0.
-
-    Example:
-        Starting Values:
-            - alleles: [‘G’, ‘*’, ‘A’, ‘C’, ‘GCTT’, ‘GT’, ‘T’]
-            - LGT: 1/2
-            - LA: [0, 1, 6]
-            - LPL: [138, 98, 154, 26, 0, 14]
-            - QUALapprox: 138
-
-        Use `extract_as_pls` to get PLs for each allele:
-            - allele 1: [138, 98, 154]
-            - allele 2: [138, 26, 14]
-
-        Normalize PLs by subtracting the smallest element from all the PLs:
-            - allele 1: [138-98, 98-98, 154-98] -> [40, 0, 56]
-            - allele 2: [138-14, 26-14, 14-14] -> [124, 12, 0]
-
-        Use the first element of the allele specific PLs to generate AS_QUALapprox:
-        [None, 40, 124]
-
-        Set QUALapprox to 0 for the star allele: [None, 0, 124]
-
-    :param mt: Input MatrixTable.
-    :return: AS_QUALapprox ArrayExpression recomputed from LPL.
-    """
-    return hl.enumerate(mt.LA).map(
-        lambda i: (
-            hl.case()
-            .when(mt.alleles[i[1]] == "*", 0)
-            .when(
-                i[0] > 0,
-                hl.bind(
-                    lambda pl_0: hl.if_else((mt.GQ < 2) & (pl_0 == 1), 0, pl_0),
-                    hl.bind(lambda x: x[0] - hl.min(x), extract_as_pls(mt.LPL, i[0])),
-                ),
-            )
-            .or_missing()
-        )
-    )
-
-
-def run_compute_info(mt: hl.MatrixTable, test: bool = False) -> hl.Table:
-    """
-    Run compute info on a MatrixTable.
-
-    ..note::
-
-        Adds a fix for AS_QUALapprox by recomputing from LPL because some were found to
-        have different lengths than LA.
-
-    :param mt: Input MatrixTable.
-    :param test: Whether to use the test dataset. Default is False.
-    :return: Table with info annotations.
-    """
-    mt = mt.annotate_entries(
-        gvcf_info=mt.gvcf_info.annotate(
-            AS_QUALapprox=recompute_as_qualapprox_from_lpl(mt)
-        )
-    )
-
-    if test:
-        unrelated_expr = ~mt.meta.rand_sampling_meta.related
-    else:
-        unrelated_expr = ~mt.meta.sample_filters.relatedness_filters.related
-
-    return default_compute_info(
-        mt,
-        site_annotations=True,
-        as_annotations=True,
-        ac_filter_groups={"release": mt.meta.release, "unrelated": unrelated_expr},
-    )
-
-
 def split_info(info_ht: hl.Table) -> hl.Table:
     """
     Generate an info Table with split multi-allelic sites from the multi-allelic info Table.
@@ -181,6 +64,37 @@
     return info_ht
 
 
+def run_generate_trio_stats(
+    vds: hl.vds.VariantDataset,
+    fam_ped: hl.Pedigree,
+    fam_ht: hl.Table,
+) -> hl.Table:
+    """
+    Generate trio transmission stats from a VariantDataset and pedigree info.
+
+    :param vds: VariantDataset to generate trio stats from.
+    :param fam_ped: Pedigree containing trio info.
+    :param fam_ht: Table containing trio info.
+    :return: Table containing trio stats.
+    """
+    # Filter the VDS to autosomes.
+    vds = hl.vds.filter_chromosomes(vds, keep_autosomes=True)
+    vmt = vds.variant_data
+    rmt = vds.reference_data
+
+    # Filter the variant data and reference data to only the trios.
+    vmt = filter_mt_to_trios(vmt, fam_ht)
+    rmt = rmt.filter_cols(hl.is_defined(vmt.cols()[rmt.col_key]))
+
+    # TODO: Should we be filtering to bi-allelic?
+    vmt = vmt.filter_rows(hl.len(vmt.alleles) == 2)
+
+    mt = hl.vds.densify(hl.vds.VariantDataset(rmt, vmt))
+    mt = hl.trio_matrix(mt, pedigree=fam_ped, complete_trios=True)
+
+    return generate_trio_stats(mt, bi_allelic_only=False)
+
+
 def get_variant_qc_annotation_resources(
     test: bool, overwrite: bool
 ) -> PipelineResourceCollection:
@@ -215,14 +129,23 @@
     )
     run_vep = PipelineStepResourceCollection(
         "--run-vep",
-        output_resources={
-            "vep_ht": get_vep(version=CURRENT_VERSION, data_type="exomes", test=test)
+        output_resources={"vep_ht": get_vep(test=test)},
+    )
+    generate_trio_stats = PipelineStepResourceCollection(
+        "--generate-trio-stats",
+        output_resources={"trio_stats_ht": get_trio_stats(test=test)},
+        input_resources={
+            "identify_trios.py --finalize-ped": {"final_ped": pedigree(test=test)}
         },
     )
-    validate_vep = PipelineStepResourceCollection(
-        "--validate-vep",
-        output_resources={"vep_count_ht": validate_vep_path(test=test)},
-        pipeline_input_steps=[run_vep],
+    generate_sib_stats = PipelineStepResourceCollection(
+        "--generate-sib-stats",
+        output_resources={"sib_stats_ht": get_sib_stats(test=test)},
+        input_resources={
+            "relatedness.py --finalize-relatedness-ht": {
+                "rel_ht": relatedness(test=test)
+            }
+        },
     )
 
     # Add all steps to the variant QC annotation pipeline resource collection.
@@ -232,7 +155,8 @@
             "split_info": split_info_ann,
             "export_info_vcf": export_info_vcf,
             "run_vep": run_vep,
-            "validate_vep": validate_vep,
+            "generate_trio_stats": generate_trio_stats,
+            "generate_sib_stats": generate_sib_stats,
         }
     )
 
@@ -252,13 +176,16 @@
     run_vep = args.run_vep
     overwrite = args.overwrite
     resources = get_variant_qc_annotation_resources(test=test, overwrite=overwrite)
-    mt = get_gnomad_v4_vds(
+    # TODO: Need to change this, reload in run_vep
+    # TODO: Can't change alleles when filter UKB samples.
+    vds = get_gnomad_v4_vds(
         test=test_dataset,
         high_quality_only=False if run_vep else True,
         # Keep control/truth samples because they are used in variant QC.
         keep_controls=False if run_vep else True,
         annotate_meta=False if run_vep else True,
-    ).variant_data
+    )
+    mt = vds.variant_data
 
     if test_n_partitions:
         mt = mt._filter_partitions(range(test_n_partitions))
@@ -267,8 +194,15 @@
         # TODO: is there any reason to also compute info per platform?
         res = resources.compute_info
         res.check_resource_existence()
-        ht = run_compute_info(mt, test=test_dataset)
-        ht.write(res.info_ht.path, overwrite=overwrite)
+        if test_dataset:
+            unrelated_expr = ~mt.meta.rand_sampling_meta.related
+        else:
+            unrelated_expr = ~mt.meta.sample_filters.relatedness_filters.related
+        default_compute_info(
+            mt,
+            site_annotations=True,
+            ac_filter_groups={"release": mt.meta.release, "unrelated": unrelated_expr},
+        ).write(res.info_ht.path, overwrite=overwrite)
 
     if args.split_info:
         res = resources.split_info
@@ -285,15 +219,19 @@
         res.check_resource_existence()
         ht = hl.split_multi(mt.rows())
         ht = vep_or_lookup_vep(ht, vep_version=args.vep_version)
-        ht.write(res.vep_ht.path, overwrite=args.overwrite)
+        ht.write(res.vep_ht.path, overwrite=overwrite)
 
-    if args.validate_vep:
-        res = resources.validate_vep
+    if args.generate_trio_stats:
+        res = resources.generate_trio_stats
         res.check_resource_existence()
-        count_ht = count_vep_annotated_variants_per_interval(
-            res.vep_ht.ht(), ensembl_interval.ht()
-        )
-        count_ht.write(res.vep_count_ht.path, overwrite=args.overwrite)
+        ht = run_generate_trio_stats(vds, res.final_ped.pedigree(), res.final_ped.ht())
+        ht.write(res.trio_stats_ht.path, overwrite=overwrite)
+
+    if args.generate_sibling_stats:
+        res = resources.generate_trio_stats
+        res.check_resource_existence()
+        ht = generate_sib_stats(mt, res.rel_ht, bi_allelic_only=False)
+        ht.write(res.sib_stats_ht.path, overwrite=overwrite)
 
 
 if __name__ == "__main__":
@@ -320,14 +258,6 @@
     parser.add_argument(
         "--run-vep", help="Generates vep annotations.", action="store_true"
     )
-    parser.add_argument(
-        "--validate-vep",
-        help=(
-            "Validate that variants in protein-coding genes are correctly annotated by"
-            " VEP."
-        ),
-        action="store_true",
-    )
     parser.add_argument(
         "--vep-version",
         help="Version of VEPed context Table to use in vep_or_lookup_vep.",
